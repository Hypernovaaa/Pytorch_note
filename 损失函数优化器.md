# 1.权值初始化
- 适当的权值初始化可以加快模型收敛，反之可能导致梯度消失或者梯度爆炸
## 1.1梯度的消失与爆炸
<img alt="损失函数优化器-374f06a2.png" src="assets/损失函数优化器-374f06a2.png" width="" height="" >当前层的梯度与上一层的输出相关
[(方差、标准差)](https://zhuanlan.zhihu.com/p/83410946)  
两个随机变量乘积的方差推导公式
<img alt="损失函数优化器-68579b89.png" src="assets/损失函数优化器-68579b89.png" width="" height="" >  
如果输入和随机变量的都是零均值1标准差的话，那么网络层的输出都会较前一层扩大 $\sqrt{n}$ 倍，输入一般会进行归一化处理，所以只能通过权值的初始化处理来维持标准差不变，避免梯度消失或者爆炸。
<img alt="损失函数优化器-95dfbee8.png" src="assets/损失函数优化器-95dfbee8.png" width="" height="" >
## 1.2具有激活函数时的初始化
- 由于加入了激活函数会破坏原本的数据分布，可能会导致梯度消失
```python
tanh_gain = nn.init.caculate_gain('tanh') # 计算tanh激活函数的增益：D（输入）/D(输出)
```

# 2.损失函数
![hengliang](assets/损失函数优化器-dec67e88.png)  
![](assets/损失函数优化器-2177da7a.png)
损失函数可以看做一个特殊的网络层，构建损失函数只在__init__中实现了self.reduction（计算loss的三种模式）。
## 2.1熵的背景知识
![](assets/损失函数优化器-b311db13.png)
其中p代表的真实概率分布，Q带表输出概率分布
- [自信息](https://zhuanlan.zhihu.com/p/26486223)：衡量随机变量为某一取值时的信息量大小
- 熵：是在确定随机变量取值之前对可能产生的信息量的期望，描述整个系统的不确定性。
- 相对熵（relative entropy）KL散度（Kullback–Leibler divergence）：衡量两个概率分布之间的差异
## 2.2 nn.CrossEntropyLoss
![](assets/损失函数优化器-15acf8a6.png)
## 2.3 nn.NLLLoss
## 2.4 nn.BCELoss


# 3.优化器
